{
  "cells": [
    {
      "id": "3cd2bf23-8765-426f-8eaa-922312c48288",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "4b166c32-010b-40c0-a4f2-41de2ddc6573"
        },
        "ExecuteTime": {
          "end_time": "2023-07-01T23:12:16.512574+00:00",
          "start_time": "2023-07-01T23:12:04.276276+00:00"
        }
      },
      "execution_count": null,
      "source": "pip install transformers",
      "outputs": []
    },
    {
      "id": "131572d3-a618-4d62-876d-9d43b623be74",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "337fd1ec-4700-4e10-9776-79868b6daf80"
        },
        "ExecuteTime": {
          "end_time": "2023-07-01T23:12:41.948746+00:00",
          "start_time": "2023-07-01T23:12:36.625207+00:00"
        }
      },
      "execution_count": null,
      "source": "pip install --upgrade accelerate",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1e255bd9-079d-4c6b-b071-2e73f836d243",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "86ab8349-b428-4b2c-8c40-6fd51aca4400"
        },
        "tags": [],
        "ExecuteTime": {
          "end_time": "2023-07-01T23:12:45.086366+00:00",
          "start_time": "2023-07-01T23:12:44.493252+00:00"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "import string\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f2d4fc0-c549-45bf-bf46-ed187ddb6089",
      "metadata": {},
      "source": [
        "### Directories and text loading\n",
        "Initially we will set the main directories and some variables regarding the characteristics of our texts.\n",
        "We set the maximum sequence length to 25, the maximun number of words in our vocabulary to 12000 and we will use 300-dimensional embeddings. Finally we load our texts from a csv. The text file is the train file of the Quora Kaggle challenge containing around 808000 sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "01882ef2-1ff4-45b5-ae4b-96d93e537953",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "4f9d3f5d-af7e-461d-ba59-264e87d2cfe9"
        },
        "tags": [],
        "ExecuteTime": {
          "end_time": "2023-07-01T23:12:58.586656+00:00",
          "start_time": "2023-07-01T23:12:58.384900+00:00"
        },
        "datalink": {
          "f1f96dfe-fa99-4fcc-9d22-955b548e3cea": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 3,
              "orig_num_rows": 5,
              "orig_size_bytes": 160,
              "truncated_num_cols": 3,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 160,
              "truncated_string_columns": []
            },
            "display_id": "f1f96dfe-fa99-4fcc-9d22-955b548e3cea",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-07-01T08:31:46.323576",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_36d091ff34c54ed7acddca45677cd53d"
          },
          "4679b315-170f-439c-98b6-7a6387606d62": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 3,
              "orig_num_rows": 5,
              "orig_size_bytes": 160,
              "truncated_num_cols": 3,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 160,
              "truncated_string_columns": []
            },
            "display_id": "4679b315-170f-439c-98b6-7a6387606d62",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-07-01T08:33:58.194404",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_779ea1025632421cab9811ac3919f528"
          },
          "70ccc09d-91df-4123-8749-67697c5c50e0": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 3,
              "orig_num_rows": 5,
              "orig_size_bytes": 160,
              "truncated_num_cols": 3,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 160,
              "truncated_string_columns": []
            },
            "display_id": "70ccc09d-91df-4123-8749-67697c5c50e0",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-07-01T08:35:58.428284",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_fb9fbd413f7944e68dbc8533dea29afd"
          },
          "3d53f45b-6606-4fa3-b701-77b9abebdd29": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 3,
              "orig_num_rows": 5,
              "orig_size_bytes": 160,
              "truncated_num_cols": 3,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 160,
              "truncated_string_columns": []
            },
            "display_id": "3d53f45b-6606-4fa3-b701-77b9abebdd29",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-07-01T23:12:58.430127",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_804472def73f4a3a84525d703aeed56a"
          }
        }
      },
      "outputs": [],
      "source": "# Define the number of rows to load (set to None to load all rows)\nnum_rows = None  # Replace with the desired number of rows to load, or set to None for all rows\n\n# Load the data\nif num_rows is None:\n    data = pd.read_csv('test_small.csv')\nelse:\n    data = pd.read_csv('test_small.csv', nrows=num_rows)\n\ndata.head()"
    },
    {
      "cell_type": "markdown",
      "id": "c22bba65-3516-42af-9121-4167fe201557",
      "metadata": {},
      "source": [
        "### Text Preprocessing\n",
        "To preprocess the text we will use the tokenizer and the text_to_sequences function from Keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4c36c857-7703-450d-953e-272ec9dba9f8",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-07-01T23:13:08.759670+00:00",
          "start_time": "2023-07-01T23:13:08.088049+00:00"
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "8dab5732-bc7f-4a0e-8bf1-898e1cc37dfd"
        },
        "tags": [],
        "jupyter": {
          "outputs_hidden": false
        },
        "datalink": {
          "3445c970-2196-4adf-9af0-7b2f122eb5ae": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 3,
              "orig_num_rows": 5,
              "orig_size_bytes": 160,
              "truncated_num_cols": 3,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 160,
              "truncated_string_columns": [
                "question1",
                "question2"
              ]
            },
            "display_id": "3445c970-2196-4adf-9af0-7b2f122eb5ae",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-07-01T23:13:08.603788",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_59ea753957494f9c99ea4dc9c974bbf2"
          }
        }
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):\n",
        "        # Lowercase the text\n",
        "        text = text.lower()\n",
        "        # Remove punctuation\n",
        "        text = re.sub(f'[{string.punctuation}]', '', text)\n",
        "    else:\n",
        "        text = ''\n",
        "    return text\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "data['question1'] = data['question1'].apply(lambda x: tokenizer.encode_plus(preprocess_text(x), truncation=True, max_length=128, padding='max_length'))\n",
        "data['question2'] = data['question2'].apply(lambda x: tokenizer.encode_plus(preprocess_text(x), truncation=True, max_length=128, padding='max_length'))\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2c1c8a7f-c22d-4868-a532-5a806fda50a1",
      "metadata": {
        "noteable": {
          "output_collection_id": "ea7a2655-2648-4c99-a735-11e5311d967e"
        },
        "tags": [],
        "ExecuteTime": {
          "end_time": "2023-07-01T23:13:15.577163+00:00",
          "start_time": "2023-07-01T23:13:15.415264+00:00"
        }
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cea2d635-bb32-42a0-b1a9-62b607ee7c8a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-07-01T23:19:38.703714+00:00",
          "start_time": "2023-07-01T23:13:20.704123+00:00"
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "d93dc1f4-86c4-4b62-b4f2-fbbeeb2ed48d"
        },
        "tags": [],
        "datalink": {
          "40a69984-22ef-4dcd-bada-9fd6eddb9f80": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 3,
              "orig_num_rows": 5,
              "orig_size_bytes": 160,
              "truncated_num_cols": 3,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 160,
              "truncated_string_columns": [
                "question1",
                "question2"
              ]
            },
            "display_id": "40a69984-22ef-4dcd-bada-9fd6eddb9f80",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-07-01T23:19:38.547342",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_df4443bb806943629a930a536620729b"
          }
        }
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Set the model in evaluation mode to deactivate the DropOut modules\n",
        "model.eval()\n",
        "\n",
        "# If you have a GPU, put everything on cuda\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def embed_text(token_ids):\n",
        "    # Convert inputs to PyTorch tensors\n",
        "    tokens_tensor = torch.tensor([token_ids]).to(device)\n",
        "\n",
        "    # Predict hidden states features for each layer\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor)\n",
        "\n",
        "    # Get the embeddings\n",
        "    embeddings = outputs.last_hidden_state\n",
        "\n",
        "    # Calculate the mean embeddings\n",
        "    mean_embeddings = torch.mean(embeddings, dim=1).cpu().numpy()\n",
        "\n",
        "    return mean_embeddings\n",
        "\n",
        "data['question1'] = data['question1'].apply(lambda x: embed_text(x['input_ids']))\n",
        "data['question2'] = data['question2'].apply(lambda x: embed_text(x['input_ids']))\n",
        "\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8a52da8f-ffb4-44c3-9e45-799487989821",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "1b488e88-24e8-40ed-a79f-a879a555f1eb"
        },
        "tags": [],
        "ExecuteTime": {
          "end_time": "2023-07-01T23:19:40.147570+00:00",
          "start_time": "2023-07-01T23:19:38.741855+00:00"
        },
        "datalink": {
          "eab9aac6-bf10-41a2-b5b4-3b4bbcd27d9f": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 3,
              "orig_num_rows": 5,
              "orig_size_bytes": 160,
              "truncated_num_cols": 3,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 160,
              "truncated_string_columns": [
                "question1",
                "question2"
              ]
            },
            "display_id": "eab9aac6-bf10-41a2-b5b4-3b4bbcd27d9f",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-07-01T23:19:39.993787",
            "user_variable_name": "",
            "variable_name": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "# Pad the sequences to a fixed length\n",
        "def pad_sequence(sequence):\n",
        "    if len(sequence) > 768:\n",
        "        return sequence[:768]\n",
        "    else:\n",
        "        return np.pad(sequence, (0, 768 - len(sequence)), 'constant')\n",
        "\n",
        "data['question1'] = data['question1'].apply(pad_sequence)\n",
        "data['question2'] = data['question2'].apply(pad_sequence)\n",
        "\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cbb70c84-6a27-4c80-a018-a73b9ed4d97d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-07-01T23:19:57.433415+00:00",
          "start_time": "2023-07-01T23:19:57.272074+00:00"
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "df95a66c-dde6-4298-ac08-5ed0f6f43921"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(768, 400)\n",
        "        self.fc21 = nn.Linear(400, 20)\n",
        "        self.fc22 = nn.Linear(400, 20)\n",
        "        self.fc3 = nn.Linear(20, 400)\n",
        "        self.fc4 = nn.Linear(400, 768)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 768))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "32b27bd3-3a2d-42ba-9abe-85cadaf11468",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "fcc86a78-a771-4c86-924a-da6fb17a788c"
        },
        "tags": [],
        "ExecuteTime": {
          "end_time": "2023-07-01T23:20:02.755292+00:00",
          "start_time": "2023-07-01T23:20:02.586026+00:00"
        }
      },
      "outputs": [],
      "source": [
        "model = VAE()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 768), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4615057c-5640-40f4-873f-551e005dbcda",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-07-01T23:27:19.892201+00:00",
          "start_time": "2023-07-01T23:20:11.448352+00:00"
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "f105baf4-b19c-40a0-9e65-836837b3a263"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data1, data2) in enumerate(zip(data['question1'], data['question2'])):\n",
        "        data1 = torch.from_numpy(data1).to(device)\n",
        "        data2 = torch.from_numpy(data2).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data1)\n",
        "        loss = loss_function(recon_batch, data2, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data1), len(data['question1']),\n",
        "                100. * batch_idx / len(data['question1']),\n",
        "                loss.item() / len(data1)))\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_loss / len(data['question1'])))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(1, 10 + 1):\n",
        "    train(epoch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8020bf3f-dc07-468b-8477-7fb95ba75a08",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-07-01T23:27:20.093977+00:00",
          "start_time": "2023-07-01T23:27:19.900598+00:00"
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "ffb16bb7-f072-4db6-b91e-c9e6dc110626"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def generate_text(epoch):\n",
        "    model.eval()\n",
        "    sample = torch.randn(64, 20).to(device)  # Move the tensor to the GPU\n",
        "    sample = model.decode(sample)\n",
        "    print('====> Generated text after epoch {}: {}'.format(epoch, sample))\n",
        "\n",
        "for epoch in range(1, 10 + 1):\n",
        "    generate_text(epoch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0dac84e-47db-4b86-80e4-85a417402e29",
      "metadata": {},
      "source": [
        "### Project and sample sentences from the latent space\n",
        "Now we build an encoder model model that takes a sentence and projects it on the latent space and a decoder model that goes from the latent space back to the text representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "67409644-0197-45b0-bba7-5b1add14b13d",
      "metadata": {
        "tags": [],
        "noteable": {
          "output_collection_id": "53099115-07e9-4e57-8ed1-9b0b175f5a04"
        },
        "ExecuteTime": {
          "end_time": "2023-07-01T23:27:37.473256+00:00",
          "start_time": "2023-07-01T23:27:37.308170+00:00"
        }
      },
      "outputs": [],
      "source": [
        "# build a model to project inputs on the latent space\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.fc1 = nn.Linear(768, 400)\n",
        "        self.fc21 = nn.Linear(400, 20)\n",
        "        self.fc22 = nn.Linear(400, 20)\n",
        "    def forward(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "# build a generator that can sample from the learned distribution\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.fc3 = nn.Linear(20, 400)\n",
        "        self.fc4 = nn.Linear(400, 768)\n",
        "    def forward(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "encoder = Encoder().to(device)\n",
        "decoder = Decoder().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02096c2d-9537-4cda-a7c1-8c17da55c44d",
      "metadata": {},
      "source": [
        "### Test on validation sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "43a18390-c881-4188-a6f3-13b589590710",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-07-01T23:27:45.869305+00:00",
          "start_time": "2023-07-01T23:27:45.151668+00:00"
        },
        "noteable": {
          "output_collection_id": "77f958f1-685e-4d99-86f2-8ea4fc2fc767"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "index2word = {v: k for k, v in tokenizer.get_vocab().items()}\n",
        "index2word[0] = 'pad'\n",
        "\n",
        "#test on a validation sentence\n",
        "sent_idx = 100\n",
        "sent_encoded = encoder(torch.from_numpy(data['question1'][sent_idx]).to(device))\n",
        "x_test_reconstructed = decoder(sent_encoded)\n",
        "reconstructed_indexes = torch.argmax(x_test_reconstructed, dim=1).cpu().numpy()\n",
        "word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n",
        "print(' '.join(word_list))\n",
        "original_sent = list(np.vectorize(index2word.get)(data['question1'][sent_idx]))\n",
        "print(' '.join(original_sent))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd188ea9-6f6f-4870-84ca-66e8ed1b56f1",
      "metadata": {
        "tags": []
      },
      "source": [
        "### Sentence processing and interpolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50224de7-dbdb-4ac2-9b6f-c22ddbb531bb",
      "metadata": {
        "noteable": {
          "output_collection_id": null
        }
      },
      "outputs": [],
      "source": [
        "# function to parse a sentence\n",
        "def sent_parse(sentence, mat_shape):\n",
        "    sequence = tokenizer.encode(sentence)\n",
        "    padded_sent = pad_sequence(sequence)\n",
        "    return padded_sent\n",
        "\n",
        "# input: encoded sentence vector\n",
        "# output: encoded sentence vector in dataset with highest cosine similarity\n",
        "def find_similar_encoding(sent_vect):\n",
        "    all_cosine = []\n",
        "    for sent in data['question1']:\n",
        "        result = 1 - spatial.distance.cosine(sent_vect, sent)\n",
        "        all_cosine.append(result)\n",
        "    data_array = np.array(all_cosine)\n",
        "    maximum = data_array.argsort()[-3:][::-1][1]\n",
        "    new_vec = data['question1'][maximum]\n",
        "    return new_vec\n",
        "\n",
        "# input: two points, integer n\n",
        "# output: n equidistant points on the line between the input points (inclusive)\n",
        "def shortest_homology(point_one, point_two, num):\n",
        "    dist_vec = point_two - point_one\n",
        "    sample = np.linspace(0, 1, num, endpoint = True)\n",
        "    hom_sample = []\n",
        "    for s in sample:\n",
        "        hom_sample.append(point_one + s * dist_vec)\n",
        "    return hom_sample\n",
        "\n",
        "# input: original dimension sentence vector\n",
        "# output: sentence text\n",
        "def print_latent_sentence(sent_vect):\n",
        "    sent_vect = torch.tensor(sent_vect).to(device)\n",
        "    sent_reconstructed = decoder(sent_vect)\n",
        "    reconstructed_indexes = torch.argmax(sent_reconstructed, dim=1).cpu().numpy()\n",
        "    word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n",
        "    print(' '.join(word_list))\n",
        "\n",
        "def new_sents_interp(sent1, sent2, n):\n",
        "    tok_sent1 = sent_parse(sent1, [MAX_SEQUENCE_LENGTH + 2])\n",
        "    tok_sent2 = sent_parse(sent2, [MAX_SEQUENCE_LENGTH + 2])\n",
        "    enc_sent1 = encoder(torch.tensor(tok_sent1).to(device))\n",
        "    enc_sent2 = encoder(torch.tensor(tok_sent2).to(device))\n",
        "    test_hom = shortest_homology(enc_sent1.detach().cpu().numpy(), enc_sent2.detach().cpu().numpy(), n)\n",
        "    for point in test_hom:\n",
        "        print_latent_sentence(point)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e07026ef-806a-4fab-8029-26158fc325c0",
      "metadata": {},
      "source": [
        "### Example\n",
        "Now we can try to parse two sentences and interpolate between them generating new sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55bf6b6e-8280-41d7-80dc-f269123fd65c",
      "metadata": {
        "noteable": {
          "output_collection_id": null
        }
      },
      "outputs": [],
      "source": [
        "sentence1='where can i find a bad restaurant'\n",
        "mysent = sent_parse(sentence1, [MAX_SEQUENCE_LENGTH + 2])\n",
        "mysent_encoded = encoder(torch.tensor(mysent).to(device))\n",
        "print_latent_sentence(mysent_encoded.detach().cpu().numpy())\n",
        "print_latent_sentence(find_similar_encoding(mysent_encoded.detach().cpu().numpy()))\n",
        "\n",
        "sentence2='where can i find an extremely good restaurant'\n",
        "mysent2 = sent_parse(sentence2, [MAX_SEQUENCE_LENGTH + 2])\n",
        "mysent_encoded2 = encoder(torch.tensor(mysent2).to(device))\n",
        "print_latent_sentence(mysent_encoded2.detach().cpu().numpy())\n",
        "print_latent_sentence(find_similar_encoding(mysent_encoded2.detach().cpu().numpy()))\n",
        "print('-----------------')\n",
        "\n",
        "new_sents_interp(sentence1, sentence2, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c62c9bd3-f387-436f-b399-0241f2413cb6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-07-01T08:10:44.652129+00:00",
          "start_time": "2023-07-01T08:10:44.488358+00:00"
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": null
        }
      },
      "outputs": [],
      "source": [
        "# function to parse a sentence\n",
        "def sent_parse(sentence, tokenizer, device):\n",
        "    sequence = tokenizer.encode_plus(sentence, return_tensors='pt')\n",
        "    return sequence['input_ids'].to(device)\n",
        "\n",
        "# input: encoded sentence vector\n",
        "# output: encoded sentence vector in dataset with highest cosine similarity\n",
        "def find_similar_encoding(sent_vect, encoded_data):\n",
        "    all_cosine = []\n",
        "    for sent in encoded_data:\n",
        "        result = 1 - spatial.distance.cosine(sent_vect.cpu().numpy(), sent.cpu().numpy())\n",
        "        all_cosine.append(result)\n",
        "    data_array = np.array(all_cosine)\n",
        "    maximum = data_array.argsort()[-3:][::-1][1]\n",
        "    new_vec = encoded_data[maximum]\n",
        "    return new_vec\n",
        "\n",
        "# input: two points, integer n\n",
        "# output: n equidistant points on the line between the input points (inclusive)\n",
        "def shortest_homology(point_one, point_two, num):\n",
        "    dist_vec = point_two - point_one\n",
        "    sample = np.linspace(0, 1, num, endpoint = True)\n",
        "    hom_sample = []\n",
        "    for s in sample:\n",
        "        hom_sample.append(point_one + s * dist_vec)\n",
        "    return hom_sample\n",
        "\n",
        "# input: original dimension sentence vector\n",
        "# output: sentence text\n",
        "def print_latent_sentence(sent_vect, decoder, index2word):\n",
        "    sent_vect = sent_vect.unsqueeze(0)\n",
        "    sent_reconstructed = decoder(sent_vect)\n",
        "    reconstructed_indexes = torch.argmax(sent_reconstructed, dim=2).squeeze().cpu().numpy()\n",
        "    word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n",
        "    print(' '.join(word_list))\n",
        "\n",
        "def new_sents_interp(sent1, sent2, n, tokenizer, device, encoder, decoder, index2word):\n",
        "    tok_sent1 = sent_parse(sent1, tokenizer, device)\n",
        "    tok_sent2 = sent_parse(sent2, tokenizer, device)\n",
        "    enc_sent1 = encoder(tok_sent1)\n",
        "    enc_sent2 = encoder(tok_sent2)\n",
        "    test_hom = shortest_homology(enc_sent1, enc_sent2, n)\n",
        "    for point in test_hom:\n",
        "        print_latent_sentence(point, decoder, index2word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "944f9619-5d6a-4318-a786-254d49ac05c8",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-07-01T08:11:28.138813+00:00",
          "start_time": "2023-07-01T08:11:27.959011+00:00"
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": null
        }
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "sentence1='where can i find a bad restaurant'\n",
        "sentence2='where can i find an extremely good restaurant'\n",
        "new_sents_interp(sentence1, sentence2, 5, tokenizer, device, encoder, decoder, index2word)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "noteable": {
      "last_delta_id": "4a3fbf04-ef34-460b-b616-055e9285c30e",
      "last_transaction_id": "e2eb2466-7530-4b23-bbd0-512a2d670440"
    },
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "b2c6ef96-6967-5cef-872b-98d4c190f96e",
        "openai_ephemeral_user_id": "4eca9769-3ab9-5591-bb64-729453b68e61",
        "openai_subdivision1_iso_code": "AU-NSW"
      }
    },
    "nteract": {
      "version": "noteable@2.9.0"
    },
    "selected_hardware_size": "small",
    "kernel_info": {
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}