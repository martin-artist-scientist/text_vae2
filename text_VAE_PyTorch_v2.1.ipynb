{
  "cells": [
    {
      "id": "3cd2bf23-8765-426f-8eaa-922312c48288",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": null
        },
        "ExecuteTime": {
          "end_time": "2023-07-01T22:58:57.560132+00:00",
          "start_time": "2023-07-01T22:58:43.790900+00:00"
        }
      },
      "execution_count": null,
      "source": "pip install transformers",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1e255bd9-079d-4c6b-b071-2e73f836d243",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": null
        },
        "tags": [],
        "ExecuteTime": {
          "end_time": "2023-07-01T10:31:43.281347+00:00",
          "start_time": "2023-07-01T10:31:42.754358+00:00"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "import string\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f2d4fc0-c549-45bf-bf46-ed187ddb6089",
      "metadata": {},
      "source": [
        "### Directories and text loading\n",
        "Initially we will set the main directories and some variables regarding the characteristics of our texts.\n",
        "We set the maximum sequence length to 25, the maximun number of words in our vocabulary to 12000 and we will use 300-dimensional embeddings. Finally we load our texts from a csv. The text file is the train file of the Quora Kaggle challenge containing around 808000 sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "01882ef2-1ff4-45b5-ae4b-96d93e537953",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": null
        },
        "tags": [],
        "ExecuteTime": {
          "end_time": "2023-07-01T08:36:10.200052+00:00",
          "start_time": "2023-07-01T08:36:10.039840+00:00"
        },
        "datalink": {
          "f1f96dfe-fa99-4fcc-9d22-955b548e3cea": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 3,
              "orig_num_rows": 5,
              "orig_size_bytes": 160,
              "truncated_num_cols": 3,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 160,
              "truncated_string_columns": []
            },
            "display_id": "f1f96dfe-fa99-4fcc-9d22-955b548e3cea",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-07-01T08:31:46.323576",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_36d091ff34c54ed7acddca45677cd53d"
          },
          "4679b315-170f-439c-98b6-7a6387606d62": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 3,
              "orig_num_rows": 5,
              "orig_size_bytes": 160,
              "truncated_num_cols": 3,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 160,
              "truncated_string_columns": []
            },
            "display_id": "4679b315-170f-439c-98b6-7a6387606d62",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-07-01T08:33:58.194404",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_779ea1025632421cab9811ac3919f528"
          },
          "70ccc09d-91df-4123-8749-67697c5c50e0": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 3,
              "orig_num_rows": 5,
              "orig_size_bytes": 160,
              "truncated_num_cols": 3,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 160,
              "truncated_string_columns": []
            },
            "display_id": "70ccc09d-91df-4123-8749-67697c5c50e0",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-07-01T08:35:58.428284",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_fb9fbd413f7944e68dbc8533dea29afd"
          }
        }
      },
      "outputs": [],
      "source": "# Define the number of rows to load (set to None to load all rows)\nnum_rows = None  # Replace with the desired number of rows to load, or set to None for all rows\n\n# Load the data\nif num_rows is None:\n    data = pd.read_csv('test_small.csv')\nelse:\n    data = pd.read_csv('test_small.csv', nrows=num_rows)\n\ndata.head()"
    },
    {
      "cell_type": "markdown",
      "id": "c22bba65-3516-42af-9121-4167fe201557",
      "metadata": {},
      "source": [
        "### Text Preprocessing\n",
        "To preprocess the text we will use the tokenizer and the text_to_sequences function from Keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4c36c857-7703-450d-953e-272ec9dba9f8",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-07-01T08:34:10.059430+00:00",
          "start_time": "2023-07-01T08:34:09.872983+00:00"
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": null
        },
        "tags": [],
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):\n",
        "        # Lowercase the text\n",
        "        text = text.lower()\n",
        "        # Remove punctuation\n",
        "        text = re.sub(f'[{string.punctuation}]', '', text)\n",
        "    else:\n",
        "        text = ''\n",
        "    return text\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "data['question1'] = data['question1'].apply(lambda x: tokenizer.encode_plus(preprocess_text(x), truncation=True, max_length=128, padding='max_length'))\n",
        "data['question2'] = data['question2'].apply(lambda x: tokenizer.encode_plus(preprocess_text(x), truncation=True, max_length=128, padding='max_length'))\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2c1c8a7f-c22d-4868-a532-5a806fda50a1",
      "metadata": {
        "noteable": {
          "output_collection_id": null
        },
        "tags": [],
        "ExecuteTime": {
          "end_time": "2023-07-01T08:34:23.017921+00:00",
          "start_time": "2023-07-01T08:34:22.840381+00:00"
        }
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cea2d635-bb32-42a0-b1a9-62b607ee7c8a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-07-01T05:30:21.180834+00:00",
          "start_time": "2023-07-01T05:30:20.995338+00:00"
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": null
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Set the model in evaluation mode to deactivate the DropOut modules\n",
        "model.eval()\n",
        "\n",
        "# If you have a GPU, put everything on cuda\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def embed_text(token_ids):\n",
        "    # Convert inputs to PyTorch tensors\n",
        "    tokens_tensor = torch.tensor([token_ids]).to(device)\n",
        "\n",
        "    # Predict hidden states features for each layer\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor)\n",
        "\n",
        "    # Get the embeddings\n",
        "    embeddings = outputs.last_hidden_state\n",
        "\n",
        "    # Calculate the mean embeddings\n",
        "    mean_embeddings = torch.mean(embeddings, dim=1).cpu().numpy()\n",
        "\n",
        "    return mean_embeddings\n",
        "\n",
        "data['question1'] = data['question1'].apply(lambda x: embed_text(x['input_ids']))\n",
        "data['question2'] = data['question2'].apply(lambda x: embed_text(x['input_ids']))\n",
        "\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8a52da8f-ffb4-44c3-9e45-799487989821",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": null
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Pad the sequences to a fixed length\n",
        "def pad_sequence(sequence):\n",
        "    if len(sequence) > 768:\n",
        "        return sequence[:768]\n",
        "    else:\n",
        "        return np.pad(sequence, (0, 768 - len(sequence)), 'constant')\n",
        "\n",
        "data['question1'] = data['question1'].apply(pad_sequence)\n",
        "data['question2'] = data['question2'].apply(pad_sequence)\n",
        "\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cbb70c84-6a27-4c80-a018-a73b9ed4d97d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-07-01T05:30:49.937836+00:00",
          "start_time": "2023-07-01T05:30:49.745850+00:00"
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": null
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(768, 400)\n",
        "        self.fc21 = nn.Linear(400, 20)\n",
        "        self.fc22 = nn.Linear(400, 20)\n",
        "        self.fc3 = nn.Linear(20, 400)\n",
        "        self.fc4 = nn.Linear(400, 768)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 768))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "32b27bd3-3a2d-42ba-9abe-85cadaf11468",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": null
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "model = VAE()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 768), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4615057c-5640-40f4-873f-551e005dbcda",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-07-01T05:31:22.629012+00:00",
          "start_time": "2023-07-01T05:31:22.427936+00:00"
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": null
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data1, data2) in enumerate(zip(data['question1'], data['question2'])):\n",
        "        data1 = torch.from_numpy(data1).to(device)\n",
        "        data2 = torch.from_numpy(data2).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data1)\n",
        "        loss = loss_function(recon_batch, data2, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data1), len(data['question1']),\n",
        "                100. * batch_idx / len(data['question1']),\n",
        "                loss.item() / len(data1)))\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_loss / len(data['question1'])))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(1, 10 + 1):\n",
        "    train(epoch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8020bf3f-dc07-468b-8477-7fb95ba75a08",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-07-01T05:31:38.889469+00:00",
          "start_time": "2023-07-01T05:31:38.697931+00:00"
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": null
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def generate_text(epoch):\n",
        "    model.eval()\n",
        "    sample = torch.randn(64, 20).to(device)  # Move the tensor to the GPU\n",
        "    sample = model.decode(sample)\n",
        "    print('====> Generated text after epoch {}: {}'.format(epoch, sample))\n",
        "\n",
        "for epoch in range(1, 10 + 1):\n",
        "    generate_text(epoch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0dac84e-47db-4b86-80e4-85a417402e29",
      "metadata": {},
      "source": [
        "### Project and sample sentences from the latent space\n",
        "Now we build an encoder model model that takes a sentence and projects it on the latent space and a decoder model that goes from the latent space back to the text representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "67409644-0197-45b0-bba7-5b1add14b13d",
      "metadata": {
        "tags": [],
        "noteable": {
          "output_collection_id": null
        }
      },
      "outputs": [],
      "source": [
        "# build a model to project inputs on the latent space\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.fc1 = nn.Linear(768, 400)\n",
        "        self.fc21 = nn.Linear(400, 20)\n",
        "        self.fc22 = nn.Linear(400, 20)\n",
        "    def forward(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "# build a generator that can sample from the learned distribution\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.fc3 = nn.Linear(20, 400)\n",
        "        self.fc4 = nn.Linear(400, 768)\n",
        "    def forward(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "encoder = Encoder().to(device)\n",
        "decoder = Decoder().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02096c2d-9537-4cda-a7c1-8c17da55c44d",
      "metadata": {},
      "source": [
        "### Test on validation sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "43a18390-c881-4188-a6f3-13b589590710",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-07-01T08:09:52.036309+00:00",
          "start_time": "2023-07-01T08:09:51.495205+00:00"
        },
        "noteable": {
          "output_collection_id": null
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "index2word = {v: k for k, v in tokenizer.get_vocab().items()}\n",
        "index2word[0] = 'pad'\n",
        "\n",
        "#test on a validation sentence\n",
        "sent_idx = 100\n",
        "sent_encoded = encoder(torch.from_numpy(data['question1'][sent_idx]).to(device))\n",
        "x_test_reconstructed = decoder(sent_encoded)\n",
        "reconstructed_indexes = torch.argmax(x_test_reconstructed, dim=1).cpu().numpy()\n",
        "word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n",
        "print(' '.join(word_list))\n",
        "original_sent = list(np.vectorize(index2word.get)(data['question1'][sent_idx]))\n",
        "print(' '.join(original_sent))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd188ea9-6f6f-4870-84ca-66e8ed1b56f1",
      "metadata": {
        "tags": []
      },
      "source": [
        "### Sentence processing and interpolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50224de7-dbdb-4ac2-9b6f-c22ddbb531bb",
      "metadata": {
        "noteable": {
          "output_collection_id": null
        }
      },
      "outputs": [],
      "source": [
        "# function to parse a sentence\n",
        "def sent_parse(sentence, mat_shape):\n",
        "    sequence = tokenizer.encode(sentence)\n",
        "    padded_sent = pad_sequence(sequence)\n",
        "    return padded_sent\n",
        "\n",
        "# input: encoded sentence vector\n",
        "# output: encoded sentence vector in dataset with highest cosine similarity\n",
        "def find_similar_encoding(sent_vect):\n",
        "    all_cosine = []\n",
        "    for sent in data['question1']:\n",
        "        result = 1 - spatial.distance.cosine(sent_vect, sent)\n",
        "        all_cosine.append(result)\n",
        "    data_array = np.array(all_cosine)\n",
        "    maximum = data_array.argsort()[-3:][::-1][1]\n",
        "    new_vec = data['question1'][maximum]\n",
        "    return new_vec\n",
        "\n",
        "# input: two points, integer n\n",
        "# output: n equidistant points on the line between the input points (inclusive)\n",
        "def shortest_homology(point_one, point_two, num):\n",
        "    dist_vec = point_two - point_one\n",
        "    sample = np.linspace(0, 1, num, endpoint = True)\n",
        "    hom_sample = []\n",
        "    for s in sample:\n",
        "        hom_sample.append(point_one + s * dist_vec)\n",
        "    return hom_sample\n",
        "\n",
        "# input: original dimension sentence vector\n",
        "# output: sentence text\n",
        "def print_latent_sentence(sent_vect):\n",
        "    sent_vect = torch.tensor(sent_vect).to(device)\n",
        "    sent_reconstructed = decoder(sent_vect)\n",
        "    reconstructed_indexes = torch.argmax(sent_reconstructed, dim=1).cpu().numpy()\n",
        "    word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n",
        "    print(' '.join(word_list))\n",
        "\n",
        "def new_sents_interp(sent1, sent2, n):\n",
        "    tok_sent1 = sent_parse(sent1, [MAX_SEQUENCE_LENGTH + 2])\n",
        "    tok_sent2 = sent_parse(sent2, [MAX_SEQUENCE_LENGTH + 2])\n",
        "    enc_sent1 = encoder(torch.tensor(tok_sent1).to(device))\n",
        "    enc_sent2 = encoder(torch.tensor(tok_sent2).to(device))\n",
        "    test_hom = shortest_homology(enc_sent1.detach().cpu().numpy(), enc_sent2.detach().cpu().numpy(), n)\n",
        "    for point in test_hom:\n",
        "        print_latent_sentence(point)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e07026ef-806a-4fab-8029-26158fc325c0",
      "metadata": {},
      "source": [
        "### Example\n",
        "Now we can try to parse two sentences and interpolate between them generating new sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55bf6b6e-8280-41d7-80dc-f269123fd65c",
      "metadata": {
        "noteable": {
          "output_collection_id": null
        }
      },
      "outputs": [],
      "source": [
        "sentence1='where can i find a bad restaurant'\n",
        "mysent = sent_parse(sentence1, [MAX_SEQUENCE_LENGTH + 2])\n",
        "mysent_encoded = encoder(torch.tensor(mysent).to(device))\n",
        "print_latent_sentence(mysent_encoded.detach().cpu().numpy())\n",
        "print_latent_sentence(find_similar_encoding(mysent_encoded.detach().cpu().numpy()))\n",
        "\n",
        "sentence2='where can i find an extremely good restaurant'\n",
        "mysent2 = sent_parse(sentence2, [MAX_SEQUENCE_LENGTH + 2])\n",
        "mysent_encoded2 = encoder(torch.tensor(mysent2).to(device))\n",
        "print_latent_sentence(mysent_encoded2.detach().cpu().numpy())\n",
        "print_latent_sentence(find_similar_encoding(mysent_encoded2.detach().cpu().numpy()))\n",
        "print('-----------------')\n",
        "\n",
        "new_sents_interp(sentence1, sentence2, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c62c9bd3-f387-436f-b399-0241f2413cb6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-07-01T08:10:44.652129+00:00",
          "start_time": "2023-07-01T08:10:44.488358+00:00"
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": null
        }
      },
      "outputs": [],
      "source": [
        "# function to parse a sentence\n",
        "def sent_parse(sentence, tokenizer, device):\n",
        "    sequence = tokenizer.encode_plus(sentence, return_tensors='pt')\n",
        "    return sequence['input_ids'].to(device)\n",
        "\n",
        "# input: encoded sentence vector\n",
        "# output: encoded sentence vector in dataset with highest cosine similarity\n",
        "def find_similar_encoding(sent_vect, encoded_data):\n",
        "    all_cosine = []\n",
        "    for sent in encoded_data:\n",
        "        result = 1 - spatial.distance.cosine(sent_vect.cpu().numpy(), sent.cpu().numpy())\n",
        "        all_cosine.append(result)\n",
        "    data_array = np.array(all_cosine)\n",
        "    maximum = data_array.argsort()[-3:][::-1][1]\n",
        "    new_vec = encoded_data[maximum]\n",
        "    return new_vec\n",
        "\n",
        "# input: two points, integer n\n",
        "# output: n equidistant points on the line between the input points (inclusive)\n",
        "def shortest_homology(point_one, point_two, num):\n",
        "    dist_vec = point_two - point_one\n",
        "    sample = np.linspace(0, 1, num, endpoint = True)\n",
        "    hom_sample = []\n",
        "    for s in sample:\n",
        "        hom_sample.append(point_one + s * dist_vec)\n",
        "    return hom_sample\n",
        "\n",
        "# input: original dimension sentence vector\n",
        "# output: sentence text\n",
        "def print_latent_sentence(sent_vect, decoder, index2word):\n",
        "    sent_vect = sent_vect.unsqueeze(0)\n",
        "    sent_reconstructed = decoder(sent_vect)\n",
        "    reconstructed_indexes = torch.argmax(sent_reconstructed, dim=2).squeeze().cpu().numpy()\n",
        "    word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n",
        "    print(' '.join(word_list))\n",
        "\n",
        "def new_sents_interp(sent1, sent2, n, tokenizer, device, encoder, decoder, index2word):\n",
        "    tok_sent1 = sent_parse(sent1, tokenizer, device)\n",
        "    tok_sent2 = sent_parse(sent2, tokenizer, device)\n",
        "    enc_sent1 = encoder(tok_sent1)\n",
        "    enc_sent2 = encoder(tok_sent2)\n",
        "    test_hom = shortest_homology(enc_sent1, enc_sent2, n)\n",
        "    for point in test_hom:\n",
        "        print_latent_sentence(point, decoder, index2word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "944f9619-5d6a-4318-a786-254d49ac05c8",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-07-01T08:11:28.138813+00:00",
          "start_time": "2023-07-01T08:11:27.959011+00:00"
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": null
        }
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "sentence1='where can i find a bad restaurant'\n",
        "sentence2='where can i find an extremely good restaurant'\n",
        "new_sents_interp(sentence1, sentence2, 5, tokenizer, device, encoder, decoder, index2word)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "noteable": {
      "last_delta_id": "4a3fbf04-ef34-460b-b616-055e9285c30e",
      "last_transaction_id": "649e9936-e1fb-4823-a1fe-7f100104c3fd"
    },
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "b2c6ef96-6967-5cef-872b-98d4c190f96e",
        "openai_ephemeral_user_id": "4eca9769-3ab9-5591-bb64-729453b68e61",
        "openai_subdivision1_iso_code": "AU-NSW"
      }
    },
    "nteract": {
      "version": "noteable@2.9.0"
    },
    "selected_hardware_size": "small",
    "kernel_info": {
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}