{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text VAE"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "d4c8ab0e-c20d-4fc8-84b9-92aa68f1c843"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "c0836474-a2b7-4b54-8093-740ce13af271"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variational Autoencoder (VAE)"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "f03ddb37-440c-4547-af0b-44b3e3e5e0ae"
    },
    {
      "cell_type": "markdown",
      "source": [
        "VAEs are probabilistic and generative models that learn a latent representation of the input data.\n",
        "By sampling from the learned distribution, we can generate new data.\n",
        "In this notebook, we will build a VAE that can generate new text after being trained on a text corpus."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "76f438fa-d8be-4482-ad6c-dcbffc90dbc9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Network"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "71333d3b-bd64-4ec3-8464-122e7c518cad"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The encoder network takes the input data and encodes it into a latent space.\n",
        "In a VAE, the encoder network does not encode the input into a fixed code,\n",
        "but rather it produces parameters of a probability distribution (mean and variance).\n",
        "The latent representation of the input is then sampled from this distribution."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "19a10eba-9584-4d1a-b3f2-5159ab5ebccb"
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, z_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, hidden_dim)\n",
        "        self.mu = nn.Linear(hidden_dim, z_dim)\n",
        "        self.var = nn.Linear(hidden_dim, z_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = torch.relu(self.linear(x))\n",
        "        z_mu = self.mu(hidden)\n",
        "        z_var = self.var(hidden)\n",
        "        return z_mu, z_var"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "38118987-0809-4fd5-b8f6-2aeda58fd837"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder Network"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "fcae3304-731a-4c4e-9b65-dcbaa5d46010"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder network takes a sample from the latent space and decodes it back into the original input space.\n",
        "In a VAE, the decoder network is also a probabilistic model: it outputs parameters of a probability distribution from which the original input is sampled."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "81b21ba8-d324-4721-82d4-084b1af778ca"
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, z_dim, hidden_dim, output_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.linear = nn.Linear(z_dim, hidden_dim)\n",
        "        self.out = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = torch.relu(self.linear(x))\n",
        "        predicted = torch.sigmoid(self.out(hidden))\n",
        "        return predicted"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "2c3dffc4-7ecc-41aa-9201-ee681f7ff4d7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variational Autoencoder (VAE) Network"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "b8b05a43-074b-4654-8c45-8f9db595ad16"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The VAE network is composed of the encoder and the decoder.\n",
        "The encoder encodes the input data into a latent distribution, a sample is drawn from this distribution, and then the decoder decodes the sample back into the original input space."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "f03c06b0-6538-41de-b8a8-79831d4679cc"
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = Encoder(input_dim, hidden_dim, z_dim)\n",
        "        self.decoder = Decoder(z_dim, hidden_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z_mu, z_var = self.encoder(x)\n",
        "        std = torch.sqrt(z_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        x_sample = eps.mul(std).add_(z_mu)\n",
        "        predicted = self.decoder(x_sample)\n",
        "        return predicted, z_mu, z_var"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "774cc8b5-b866-4b1a-84d1-53326aa96437"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "b05ccdd4-aa2e-4b1b-9be2-c32d2f0e3263"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a VAE, we need to define a loss function that takes into account both the reconstruction loss and the KL divergence.\n",
        "The reconstruction loss measures how well the VAE can reconstruct the original input from the latent sample,\n",
        "and the KL divergence measures how closely the learned latent distribution matches the prior distribution (a standard normal distribution in this case)."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "6e5f8595-d2ae-48ba-9a99-3dfeda30e50f"
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(x, reconstructed_x, mean, log_var):\n",
        "    # Reconstruction loss\n",
        "    RCL = F.binary_cross_entropy(reconstructed_x, x, reduction='sum')\n",
        "    # KL divergence loss\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
        "    return RCL + KLD"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "a11f76b6-01fe-4e3a-b51b-25aa9aeef1d4"
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (x, _) in enumerate(dataloader):\n",
        "            x = x.view(-1, 784)\n",
        "            x_reconstructed, mean, log_var = model(x)\n",
        "            loss = calculate_loss(x, x_reconstructed, mean, log_var)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if (i+1) % 100 == 0:\n",
        "                print(f'Epoch[{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item()}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "0f3e4960-c9f1-4ca1-bf2d-1afb73000f8e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "97f6e0a4-d008-40f4-8119-f5520605a3a7"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "649cb36c-b215-423a-9168-7556f6c61560"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "ae7b878b-0f47-43c0-931d-0c364ed708c0"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "396d0e92-bb7b-47fc-893c-2f6ce0de3434"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Directories and Text Loading"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "75d32f12-6796-4410-aaeb-248e6288320a"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Quora dataset\n",
        "quora_data = pd.read_csv('test.csv')\n",
        "\n",
        "# Combine the question1 and question2 columns into a single list of documents\n",
        "documents = quora_data['question1'].tolist() + quora_data['question2'].tolist()\n",
        "\n",
        "# Remove any NaN values\n",
        "documents = [doc for doc in documents if str(doc) != 'nan']"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "3288ba41-e488-4a8e-9ca4-4b783f0f8d79"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "b6faccea-bade-4ba2-b0e2-8c26c3f6cd04"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Download the necessary NLTK corpora\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove punctuation and convert to lowercase\n",
        "    words = [word.lower() for word in words if word.isalpha()]\n",
        "\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Lemmatize the words\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return words\n",
        "\n",
        "# Preprocess the documents\n",
        "documents = [preprocess_text(doc) for doc in documents]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "ba9293ea-cdd0-494c-96df-13f2b84c72a6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embeddings"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "201b7b19-4fbb-48d3-b2b8-99aecb562bf5"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import Embedding\n",
        "from collections import Counter\n",
        "\n",
        "# Count the number of occurrences of each word\n",
        "word_counts = Counter(word for doc in documents for word in doc)\n",
        "\n",
        "# Create a dictionary that maps each word to a unique index\n",
        "word_to_index = {word: i for i, (word, _) in enumerate(word_counts.most_common())}\n",
        "\n",
        "# Add special tokens for unknown and padding\n",
        "word_to_index['<unk>'] = len(word_to_index)\n",
        "word_to_index['<pad>'] = len(word_to_index)\n",
        "\n",
        "# Create the inverse mapping, from indices to words\n",
        "index_to_word = {i: word for word, i in word_to_index.items()}\n",
        "\n",
        "# Create the embedding layer\n",
        "embedding = Embedding(len(word_to_index), 100)\n",
        "\n",
        "# Convert the documents to sequences of word indices\n",
        "documents = [[word_to_index.get(word, word_to_index['<unk>']) for word in doc] for doc in documents]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "326c54a4-fb0a-424a-a563-2e07a76b9738"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loader"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "8495aa0e-8184-4035-95bf-99be17210e6c"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, documents):\n",
        "        self.documents = documents\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.documents)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.documents[idx])\n",
        "\n",
        "# Create a function to pad sequences\n",
        "def pad_collate(batch):\n",
        "    return pad_sequence(batch, padding_value=word_to_index['<pad>'])\n",
        "\n",
        "# Create the data loader\n",
        "dataset = TextDataset(documents)\n",
        "data_loader = DataLoader(dataset, batch_size=64, collate_fn=pad_collate)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "b058e4e3-a49d-4bdb-a328-45d759db5095"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variational Autoencoder"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "6116e169-792e-4e1c-ae21-51cc64d42dde"
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder_rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc_var = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder_rnn = nn.GRU(latent_dim, hidden_dim, batch_first=True)\n",
        "        self.decoder_out = nn.Linear(hidden_dim, embedding_dim)\n",
        "\n",
        "    def encode(self, x):\n",
        "        _, h = self.encoder_rnn(x)\n",
        "        h = h.squeeze(0)\n",
        "        mu = self.fc_mu(h)\n",
        "        log_var = self.fc_var(h)\n",
        "        return mu, log_var\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5*log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = z.unsqueeze(1)\n",
        "        h = self.decoder_rnn(z)\n",
        "        return self.decoder_out(h)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encode(x)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        return self.decode(z), mu, log_var"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "ad50300c-63b3-4fae-aed1-f498bd76003a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Function and Training Loop"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "35c0bf4a-e978-4416-96ff-666d63d105df"
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(x, reconstructed_x, mean, log_var):\n",
        "    # Reconstruction loss\n",
        "    RCL = F.mse_loss(reconstructed_x, x, reduction='sum')\n",
        "    # KL divergence\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
        "    return RCL + KLD\n",
        "\n",
        "def train(model, dataloader, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, x in enumerate(dataloader):\n",
        "            x = x.float()\n",
        "            x_reconstructed, mean, log_var = model(x)\n",
        "            loss = calculate_loss(x, x_reconstructed, mean, log_var)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if (i+1) % 100 == 0:\n",
        "                print(f'Epoch[{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item()}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "fc116f3e-558f-4dd4-88f9-815bc121cead"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "7447b0f0-de41-4beb-b821-080534c8ccf2"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "ff5abfdb-5243-4099-a008-4319a0f907d1"
    }
  ],
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "b2c6ef96-6967-5cef-872b-98d4c190f96e",
        "openai_ephemeral_user_id": "4eca9769-3ab9-5591-bb64-729453b68e61",
        "openai_subdivision1_iso_code": "AU-NSW"
      }
    },
    "noteable": {
      "last_transaction_id": "f92ca70e-d972-4d3d-bdc9-d5877047d06d",
      "last_delta_id": "a17a5453-dd04-486e-a33c-31c92cb028a0"
    },
    "selected_hardware_size": "small",
    "nteract": {
      "version": "noteable@2.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}